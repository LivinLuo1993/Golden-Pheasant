{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABapp9JcEvsb",
        "outputId": "edbf43a8-8e19-4849-dcf7-f40207fd9d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 11 02:32:51 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5scPAQ7_Grt6",
        "outputId": "7dbe24d9-2924-494c-d5bc-9245bcff5a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/AIGC/chatrwkv\n"
          ]
        }
      ],
      "source": [
        "# 配置云盘\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# 修改工作路径\n",
        "%cd drive/MyDrive/AIGC/chatrwkv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir raven\n",
        "%cd raven\n",
        "# 7B中文主体模型\n",
        "!wget https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048.pth\n",
        "# 14B英文主体模型\n",
        "!wget https://huggingface.co/BlinkDL/rwkv-4-raven/resolve/main/RWKV-4-Raven-14B-v8-EngAndMore-20230408-ctx4096.pth"
      ],
      "metadata": {
        "id": "Q3xuzb5AHSDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rwkv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRr3gZLKJAdp",
        "outputId": "615fb426-bdd6-4600-ecc0-a6161b9d95bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rwkv\n",
            "  Downloading rwkv-0.7.3-py3-none-any.whl (16 kB)\n",
            "Collecting tokenizers>=0.13.2\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, rwkv\n",
            "Successfully installed rwkv-0.7.3 tokenizers-0.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
        "os.environ[\"RWKV_CUDA_ON\"] = '0' # if '1' then use CUDA kernel for seq mode (much faster)\n",
        "from rwkv.model import RWKV                         # pip install rwkv\n",
        "model = RWKV(model='/content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048', strategy='cuda fp16i8')\n",
        "\n",
        "out, state = model.forward([187, 510, 1563, 310, 247], None)   # use 20B_tokenizer.json\n",
        "print(out.detach().cpu().numpy())                   # get logits"
      ],
      "metadata": {
        "id": "O3RvhIK7LTtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17d3aa0-7392-4e1c-ba42-f54a842c7975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 6\n",
            "\n",
            "Loading /content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048.pth ...\n",
            "Strategy: (total 32+1=33 layers)\n",
            "* cuda [float16, uint8], store 33 layers\n",
            "0-cuda-float16-uint8 1-cuda-float16-uint8 2-cuda-float16-uint8 3-cuda-float16-uint8 4-cuda-float16-uint8 5-cuda-float16-uint8 6-cuda-float16-uint8 7-cuda-float16-uint8 8-cuda-float16-uint8 9-cuda-float16-uint8 10-cuda-float16-uint8 11-cuda-float16-uint8 12-cuda-float16-uint8 13-cuda-float16-uint8 14-cuda-float16-uint8 15-cuda-float16-uint8 16-cuda-float16-uint8 17-cuda-float16-uint8 18-cuda-float16-uint8 19-cuda-float16-uint8 20-cuda-float16-uint8 21-cuda-float16-uint8 22-cuda-float16-uint8 23-cuda-float16-uint8 24-cuda-float16-uint8 25-cuda-float16-uint8 26-cuda-float16-uint8 27-cuda-float16-uint8 28-cuda-float16-uint8 29-cuda-float16-uint8 30-cuda-float16-uint8 31-cuda-float16-uint8 32-cuda-float16-uint8 \n",
            "emb.weight                        f16      cpu  50277  4096 \n",
            "blocks.0.ln1.weight               f16   cuda:0   4096       \n",
            "blocks.0.ln1.bias                 f16   cuda:0   4096       \n",
            "blocks.0.ln2.weight               f16   cuda:0   4096       \n",
            "blocks.0.ln2.bias                 f16   cuda:0   4096       \n",
            "blocks.0.att.time_decay           f32   cuda:0   4096       \n",
            "blocks.0.att.time_first           f32   cuda:0   4096       \n",
            "blocks.0.att.time_mix_k           f16   cuda:0   4096       \n",
            "blocks.0.att.time_mix_v           f16   cuda:0   4096       \n",
            "blocks.0.att.time_mix_r           f16   cuda:0   4096       \n",
            "blocks.0.att.key.weight            i8   cuda:0   4096  4096 \n",
            "blocks.0.att.value.weight          i8   cuda:0   4096  4096 \n",
            "blocks.0.att.receptance.weight     i8   cuda:0   4096  4096 \n",
            "blocks.0.att.output.weight         i8   cuda:0   4096  4096 \n",
            "blocks.0.ffn.time_mix_k           f16   cuda:0   4096       \n",
            "blocks.0.ffn.time_mix_r           f16   cuda:0   4096       \n",
            "blocks.0.ffn.key.weight            i8   cuda:0   4096 16384 \n",
            "blocks.0.ffn.receptance.weight     i8   cuda:0   4096  4096 \n",
            "blocks.0.ffn.value.weight          i8   cuda:0  16384  4096 \n",
            "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "blocks.31.ln1.weight              f16   cuda:0   4096       \n",
            "blocks.31.ln1.bias                f16   cuda:0   4096       \n",
            "blocks.31.ln2.weight              f16   cuda:0   4096       \n",
            "blocks.31.ln2.bias                f16   cuda:0   4096       \n",
            "blocks.31.att.time_decay          f32   cuda:0   4096       \n",
            "blocks.31.att.time_first          f32   cuda:0   4096       \n",
            "blocks.31.att.time_mix_k          f16   cuda:0   4096       \n",
            "blocks.31.att.time_mix_v          f16   cuda:0   4096       \n",
            "blocks.31.att.time_mix_r          f16   cuda:0   4096       \n",
            "blocks.31.att.key.weight           i8   cuda:0   4096  4096 \n",
            "blocks.31.att.value.weight         i8   cuda:0   4096  4096 \n",
            "blocks.31.att.receptance.weight    i8   cuda:0   4096  4096 \n",
            "blocks.31.att.output.weight        i8   cuda:0   4096  4096 \n",
            "blocks.31.ffn.time_mix_k          f16   cuda:0   4096       \n",
            "blocks.31.ffn.time_mix_r          f16   cuda:0   4096       \n",
            "blocks.31.ffn.key.weight           i8   cuda:0   4096 16384 \n",
            "blocks.31.ffn.receptance.weight    i8   cuda:0   4096  4096 \n",
            "blocks.31.ffn.value.weight         i8   cuda:0  16384  4096 \n",
            "ln_out.weight                     f16   cuda:0   4096       \n",
            "ln_out.bias                       f16   cuda:0   4096       \n",
            "head.weight                        i8   cuda:0   4096 50277 \n",
            "[ -5.3164 -33.5312  -7.7812 ...  -6.1758  -3.8535  -2.0547]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################\n",
        "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
        "########################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
        "import types, torch\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/MyDrive/AIGC/chatrwkv/raven/20B_tokenizer.json\")\n",
        "\n",
        "args = types.SimpleNamespace()\n",
        "args.MODEL_NAME = '/content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048'\n",
        "args.n_layer = 24\n",
        "args.n_embd = 1024\n",
        "\n",
        "context = \"\\nIn a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\"\n",
        "NUM_TRIALS = 3\n",
        "LENGTH_PER_TRIAL = 100\n",
        "TEMPERATURE = 1.0\n",
        "TOP_P = 0.85\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "class RWKV_RNN(torch.jit.ScriptModule):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.eval() # set torch to inference mode\n",
        "        \n",
        "        w = torch.load(args.MODEL_NAME + '.pth', map_location='cuda')\n",
        "        for k in w.keys():\n",
        "            if      '.time_' in k: w[k] = w[k].squeeze()\n",
        "            if '.time_decay' in k: w[k] = -torch.exp(w[k].float()) # the real time decay is like e^{-e^x}\n",
        "            else: w[k] = w[k].float() # convert to f32 type\n",
        "        \n",
        "        self.w = types.SimpleNamespace() # set self.w from w\n",
        "        self.w.blocks = {}\n",
        "        for k in w.keys(): # example: \"blocks.0.att.time_first\" => self.w.blocks[0].att.time_first\n",
        "            parts = k.split('.')\n",
        "            last = parts.pop()\n",
        "            here = self.w\n",
        "            for p in parts:\n",
        "                if p.isdigit():\n",
        "                    p = int(p)\n",
        "                    if p not in here: here[p] = types.SimpleNamespace()\n",
        "                    here = here[p]\n",
        "                else:\n",
        "                    if not hasattr(here, p): setattr(here, p, types.SimpleNamespace())\n",
        "                    here = getattr(here, p)\n",
        "            setattr(here, last, w[k])\n",
        "\n",
        "    def layer_norm(self, x, w):\n",
        "        return F.layer_norm(x, (self.args.n_embd,), weight=w.weight, bias=w.bias)\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def channel_mixing(self, x, state, i:int, time_mix_k, time_mix_r, kw, vw, rw):\n",
        "        xk = x * time_mix_k + state[5*i+0] * (1 - time_mix_k)\n",
        "        xr = x * time_mix_r + state[5*i+0] * (1 - time_mix_r)\n",
        "        state[5*i+0] = x\n",
        "        r = torch.sigmoid(rw @ xr)\n",
        "        k = torch.square(torch.relu(kw @ xk)) # square relu, primer paper\n",
        "        return r * (vw @ k)\n",
        "\n",
        "    @torch.jit.script_method\n",
        "    def time_mixing(self, x, state, i:int, time_mix_k, time_mix_v, time_mix_r, time_first, time_decay, kw, vw, rw, ow):\n",
        "        xk = x * time_mix_k + state[5*i+1] * (1 - time_mix_k)\n",
        "        xv = x * time_mix_v + state[5*i+1] * (1 - time_mix_v)\n",
        "        xr = x * time_mix_r + state[5*i+1] * (1 - time_mix_r)\n",
        "        state[5*i+1] = x\n",
        "        r = torch.sigmoid(rw @ xr)\n",
        "        k = kw @ xk\n",
        "        v = vw @ xv\n",
        "        \n",
        "        aa = state[5*i+2]\n",
        "        bb = state[5*i+3]\n",
        "        pp = state[5*i+4]\n",
        "        ww = time_first + k\n",
        "        qq = torch.maximum(pp, ww)\n",
        "        e1 = torch.exp(pp - qq)\n",
        "        e2 = torch.exp(ww - qq)\n",
        "        a = e1 * aa + e2 * v\n",
        "        b = e1 * bb + e2\n",
        "        wkv = a / b\n",
        "        ww = pp + time_decay\n",
        "        qq = torch.maximum(ww, k)\n",
        "        e1 = torch.exp(ww - qq)\n",
        "        e2 = torch.exp(k - qq)\n",
        "        state[5*i+2] = e1 * aa + e2 * v\n",
        "        state[5*i+3] = e1 * bb + e2\n",
        "        state[5*i+4] = qq\n",
        "        return ow @ (r * wkv)\n",
        "\n",
        "    def forward(self, token, state):\n",
        "        with torch.no_grad():\n",
        "            if state == None:\n",
        "                state = torch.zeros(self.args.n_layer * 5, self.args.n_embd)\n",
        "                for i in range(self.args.n_layer): state[5*i+4] = -1e30 # -infinity\n",
        "            \n",
        "            x = self.w.emb.weight[token]\n",
        "            x = self.layer_norm(x, self.w.blocks[0].ln0)\n",
        "            for i in range(self.args.n_layer):\n",
        "                att = self.w.blocks[i].att\n",
        "                x = x + self.time_mixing(self.layer_norm(x, self.w.blocks[i].ln1), state, i, \n",
        "                    att.time_mix_k, att.time_mix_v, att.time_mix_r, att.time_first, att.time_decay, \n",
        "                    att.key.weight, att.value.weight, att.receptance.weight, att.output.weight)\n",
        "                ffn = self.w.blocks[i].ffn\n",
        "                x = x + self.channel_mixing(self.layer_norm(x, self.w.blocks[i].ln2), state, i, \n",
        "                    ffn.time_mix_k, ffn.time_mix_r, \n",
        "                    ffn.key.weight, ffn.value.weight, ffn.receptance.weight)\n",
        "            \n",
        "            x = self.w.head.weight @ self.layer_norm(x, self.w.ln_out)\n",
        "            return x.float(), state\n",
        "\n",
        "##########################################################################################################\n",
        "\n",
        "def sample_logits(out, temperature=1.0, top_p=0.8):\n",
        "    probs = F.softmax(out, dim=-1).numpy()\n",
        "    sorted_probs = np.sort(probs)[::-1]\n",
        "    cumulative_probs = np.cumsum(sorted_probs)\n",
        "    cutoff = float(sorted_probs[np.argmax(cumulative_probs > top_p)])\n",
        "    probs[probs < cutoff] = 0\n",
        "    if temperature != 1.0:\n",
        "        probs = probs.pow(1.0 / temperature)\n",
        "    probs = probs / np.sum(probs)\n",
        "    out = np.random.choice(a=len(probs), p=probs)\n",
        "    return out\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "print(f'\\nUsing CPU. Loading {args.MODEL_NAME} ...')\n",
        "model = RWKV_RNN(args)\n",
        "\n",
        "print(f'\\nPreprocessing context (slow version. see v2/rwkv/model.py for fast version)')\n",
        "init_state = None\n",
        "for token in tokenizer.encode(context).ids:\n",
        "    init_out, init_state = model.forward(token, init_state)\n",
        "\n",
        "for TRIAL in range(NUM_TRIALS):\n",
        "    print(f'\\n\\n--[ Trial {TRIAL} ]-----------------', context, end=\"\")\n",
        "    all_tokens = []\n",
        "    out_last = 0\n",
        "    out, state = init_out.clone(), init_state.clone()\n",
        "    for i in range(LENGTH_PER_TRIAL):\n",
        "        token = sample_logits(out, TEMPERATURE, TOP_P)\n",
        "        all_tokens += [token]\n",
        "        tmp = tokenizer.decode(all_tokens[out_last:])\n",
        "        if '\\ufffd' not in tmp: # only print when we have a valid utf-8 string\n",
        "            print(tmp, end=\"\", flush=True)\n",
        "            out_last = i + 1\n",
        "        out, state = model.forward(token, state)       \n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "h6cLxyRNFYKt",
        "outputId": "f8aa0ebd-03de-4017-abe3-b022c2315697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using CPU. Loading /content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048 ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-84b7ade9037f>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nUsing CPU. Loading {args.MODEL_NAME} ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRWKV_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nPreprocessing context (slow version. see v2/rwkv/model.py for fast version)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/jit/_script.py\u001b[0m in \u001b[0;36minit_then_script\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minit_then_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mnum_methods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_methods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0moriginal_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0madded_methods_in_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_methods\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnum_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-84b7ade9037f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m      \u001b[0;34m'.time_'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'.time_decay'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the real time decay is like e^{-e^x}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convert to f32 type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set self.w from w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 39.56 GiB total capacity; 38.36 GiB already allocated; 254.56 MiB free; 38.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3f7nVYI4NeR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/saharNooby/rwkv.cpp.git\n",
        "%cd rwkv.cpp\n",
        "\n",
        "!cmake -DBUILD_SHARED_LIBS=ON .\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WyREefNhUy",
        "outputId": "89544173-aa64-43b2-da21-051a94b30f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AIGC/chatrwkv/rwkv.cpp\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/drive/MyDrive/AIGC/chatrwkv/rwkv.cpp\n",
            "[-33%] Built target ggml\n",
            "[ 33%] Built target rwkv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将pth格式转换为ggml\n",
        "!python rwkv/convert_pytorch_to_ggml.py /content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048.pth rwkv.cpp-Raven-7B.bin float16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3R81YmDO39Q",
        "outputId": "b6e82d33-e6b8-413c-c073-aa0e7e1682cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/drive/MyDrive/AIGC/chatrwkv/raven/RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048.pth\n",
            "Writing emb.weight, shape torch.Size([50277, 4096]), type torch.float16\n",
            "Writing blocks.0.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ln0.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ln0.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.0.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.0.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.0.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.0.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.0.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.0.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.0.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.1.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.1.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.1.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.1.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.1.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.1.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.1.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.1.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.2.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.2.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.2.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.2.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.2.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.2.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.2.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.2.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.3.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.3.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.3.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.3.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.3.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.3.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.3.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.3.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.4.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.4.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.4.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.4.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.4.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.4.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.4.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.4.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.5.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.5.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.5.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.5.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.5.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.5.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.5.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.5.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.6.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.6.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.6.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.6.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.6.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.6.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.6.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.6.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.7.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.7.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.7.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.7.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.7.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.7.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.7.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.7.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.8.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.8.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.8.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.8.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.8.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.8.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.8.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.8.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.9.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.9.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.9.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.9.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.9.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.9.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.9.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.9.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.10.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.10.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.10.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.10.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.10.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.10.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.10.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.10.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.11.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.11.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.11.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.11.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.11.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.11.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.11.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.11.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.12.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.12.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.12.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.12.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.12.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.12.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.12.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.12.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.13.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.13.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.13.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.13.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.13.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.13.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.13.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.13.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.14.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.14.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.14.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.14.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.14.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.14.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.14.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.14.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.15.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.15.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.15.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.15.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.15.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.15.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.15.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.15.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.16.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.16.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.16.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.16.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.16.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.16.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.16.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.16.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.17.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.17.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.17.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.17.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.17.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.17.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.17.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.17.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.18.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.18.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.18.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.18.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.18.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.18.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.18.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.18.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.19.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.19.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.19.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.19.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.19.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.19.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.19.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.19.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.20.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.20.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.20.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.20.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.20.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.20.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.20.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.20.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.21.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.21.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.21.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.21.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.21.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.21.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.21.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.21.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.22.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.22.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.22.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.22.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.22.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.22.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.22.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.22.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.23.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.23.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.23.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.23.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.23.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.23.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.23.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.23.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.24.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.24.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.24.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.24.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.24.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.24.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.24.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.24.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.25.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.25.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.25.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.25.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.25.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.25.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.25.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.25.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.26.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.26.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.26.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.26.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.26.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.26.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.26.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.26.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.27.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.27.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.27.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.27.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.27.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.27.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.27.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.27.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.28.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.28.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.28.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.28.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.28.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.28.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.28.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.28.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.29.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.29.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.29.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.29.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.29.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.29.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.29.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.29.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.30.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.30.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.30.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.30.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.30.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.30.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.30.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.30.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing blocks.31.ln1.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.ln1.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.ln2.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.ln2.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.time_decay, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.time_first, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_v, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.att.key.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.31.att.value.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.31.att.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.31.att.output.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.31.ffn.time_mix_k, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.ffn.time_mix_r, shape torch.Size([4096]), type torch.float32\n",
            "Writing blocks.31.ffn.key.weight, shape torch.Size([16384, 4096]), type torch.float16\n",
            "Writing blocks.31.ffn.receptance.weight, shape torch.Size([4096, 4096]), type torch.float16\n",
            "Writing blocks.31.ffn.value.weight, shape torch.Size([4096, 16384]), type torch.float16\n",
            "Writing ln_out.weight, shape torch.Size([4096]), type torch.float32\n",
            "Writing ln_out.bias, shape torch.Size([4096]), type torch.float32\n",
            "Writing head.weight, shape torch.Size([50277, 4096]), type torch.float16\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型量化\n",
        "!python rwkv/quantize.py rwkv.cpp-Raven-7B.bin rwkv.cpp-Raven-7B-Q4_1_O.bin 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEp9CyoiQjBw",
        "outputId": "cb6b384e-9767-4f1a-ca1c-0bb365ce0ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from 'rwkv.cpp-Raven-7B.bin'\n",
            "                                      emb.weight - [ 4096, 50277], type =    F16 size =  392.789 MB\n",
            "                             blocks.0.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.0.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.0.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.0.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.0.ln0.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.0.ln0.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.0.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.0.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.053 0.065 0.077 0.086 0.091 0.091 0.086 0.077 0.066 0.053 0.041 0.030 0.043 \n",
            "                      blocks.0.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                         blocks.0.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.0.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.0.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                       blocks.0.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.049 0.061 0.074 0.088 0.105 0.105 0.088 0.073 0.061 0.049 0.038 0.028 0.042 \n",
            "                             blocks.1.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.1.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.1.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.1.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.078 0.065 0.052 0.039 0.028 0.042 \n",
            "                       blocks.1.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                  blocks.1.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.1.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.026 0.035 0.047 0.061 0.077 0.093 0.105 0.105 0.093 0.077 0.061 0.047 0.035 0.026 0.041 \n",
            "                         blocks.1.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.1.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.1.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                       blocks.1.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.071 0.022 0.029 0.036 0.046 0.062 0.097 0.152 0.152 0.097 0.062 0.046 0.036 0.029 0.022 0.040 \n",
            "                             blocks.2.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.2.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.2.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.2.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                       blocks.2.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.027 0.038 0.051 0.064 0.078 0.089 0.095 0.096 0.089 0.078 0.065 0.051 0.038 0.027 0.042 \n",
            "                  blocks.2.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.089 0.095 0.095 0.089 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                      blocks.2.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.072 0.024 0.033 0.044 0.058 0.075 0.096 0.113 0.113 0.096 0.075 0.058 0.044 0.033 0.025 0.041 \n",
            "                         blocks.2.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.2.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.2.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.2.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.037 0.048 0.059 0.072 0.088 0.110 0.111 0.088 0.072 0.059 0.048 0.037 0.028 0.042 \n",
            "                             blocks.3.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.3.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.3.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.3.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.028 0.042 \n",
            "                       blocks.3.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                  blocks.3.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.077 0.087 0.093 0.093 0.088 0.078 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.3.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.072 0.024 0.033 0.043 0.057 0.075 0.097 0.116 0.116 0.097 0.075 0.057 0.043 0.033 0.024 0.041 \n",
            "                         blocks.3.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.3.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.3.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.3.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.036 0.046 0.057 0.070 0.089 0.117 0.117 0.089 0.070 0.057 0.046 0.036 0.027 0.042 \n",
            "                             blocks.4.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.4.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.4.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.4.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.4.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.4.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.078 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.4.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.027 0.037 0.049 0.062 0.077 0.091 0.101 0.101 0.091 0.076 0.062 0.048 0.037 0.027 0.042 \n",
            "                         blocks.4.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.4.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.4.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.4.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.050 0.062 0.074 0.087 0.101 0.102 0.087 0.074 0.062 0.050 0.039 0.028 0.042 \n",
            "                             blocks.5.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.5.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.5.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.5.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.5.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.5.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.5.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.050 0.064 0.077 0.089 0.096 0.096 0.089 0.077 0.064 0.050 0.038 0.028 0.042 \n",
            "                         blocks.5.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.5.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.5.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.5.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.039 0.051 0.063 0.075 0.087 0.098 0.099 0.087 0.075 0.063 0.051 0.039 0.029 0.042 \n",
            "                             blocks.6.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.6.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.6.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.6.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.6.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.6.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.6.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                         blocks.6.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.6.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.6.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.6.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.039 0.051 0.063 0.075 0.087 0.099 0.099 0.087 0.075 0.063 0.051 0.039 0.029 0.042 \n",
            "                             blocks.7.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.7.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.7.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.7.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.7.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.7.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.7.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.077 0.088 0.094 0.094 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                         blocks.7.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.7.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.7.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.7.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.050 0.062 0.075 0.087 0.101 0.101 0.088 0.075 0.062 0.050 0.038 0.028 0.042 \n",
            "                             blocks.8.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.8.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.8.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.8.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.8.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.8.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.8.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                         blocks.8.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.8.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.8.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.8.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.036 0.047 0.059 0.073 0.090 0.110 0.111 0.090 0.073 0.059 0.047 0.036 0.027 0.042 \n",
            "                             blocks.9.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.9.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                             blocks.9.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                               blocks.9.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                       blocks.9.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                  blocks.9.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.9.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                         blocks.9.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                         blocks.9.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                  blocks.9.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                       blocks.9.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.035 0.046 0.058 0.073 0.091 0.114 0.114 0.091 0.072 0.058 0.046 0.035 0.026 0.041 \n",
            "                            blocks.10.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.10.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.10.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.10.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.10.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.087 0.093 0.094 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.10.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.10.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.088 0.094 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                        blocks.10.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.10.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.10.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.10.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.036 0.047 0.059 0.073 0.091 0.110 0.111 0.091 0.073 0.059 0.047 0.036 0.026 0.041 \n",
            "                            blocks.11.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.11.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.11.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.11.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.11.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.087 0.093 0.094 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                 blocks.11.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.11.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.088 0.094 0.095 0.088 0.078 0.064 0.051 0.039 0.028 0.042 \n",
            "                        blocks.11.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.11.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.11.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.11.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.035 0.047 0.059 0.074 0.091 0.110 0.110 0.091 0.074 0.059 0.047 0.036 0.026 0.041 \n",
            "                            blocks.12.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.12.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.12.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.12.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.12.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.088 0.093 0.093 0.088 0.077 0.065 0.051 0.039 0.029 0.042 \n",
            "                 blocks.12.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.12.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.064 0.051 0.039 0.028 0.042 \n",
            "                        blocks.12.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.12.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.12.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.12.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.036 0.047 0.060 0.075 0.092 0.108 0.108 0.092 0.074 0.060 0.047 0.036 0.026 0.042 \n",
            "                            blocks.13.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.13.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.13.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.13.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.13.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.088 0.093 0.094 0.088 0.077 0.065 0.051 0.039 0.029 0.042 \n",
            "                 blocks.13.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.13.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.13.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.13.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.13.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.13.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.036 0.047 0.060 0.075 0.092 0.106 0.107 0.092 0.075 0.060 0.047 0.036 0.026 0.041 \n",
            "                            blocks.14.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.14.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.14.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.14.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.14.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.088 0.093 0.093 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.14.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.14.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.088 0.094 0.095 0.088 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.14.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.14.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.14.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.14.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.026 0.035 0.047 0.060 0.075 0.093 0.108 0.108 0.093 0.075 0.060 0.046 0.035 0.026 0.041 \n",
            "                            blocks.15.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.15.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.15.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.15.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.15.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.088 0.094 0.095 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.15.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.078 0.065 0.052 0.039 0.029 0.042 \n",
            "                     blocks.15.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.15.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.15.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.15.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.15.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.037 0.049 0.062 0.076 0.090 0.101 0.101 0.091 0.076 0.062 0.049 0.037 0.027 0.042 \n",
            "                            blocks.16.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.16.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.16.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.16.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.16.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.088 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                 blocks.16.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                     blocks.16.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                        blocks.16.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.16.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.16.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.16.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.038 0.050 0.063 0.077 0.090 0.098 0.098 0.090 0.077 0.063 0.050 0.038 0.027 0.042 \n",
            "                            blocks.17.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.17.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.17.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.17.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.17.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                 blocks.17.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                     blocks.17.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.050 0.064 0.078 0.089 0.096 0.096 0.089 0.078 0.064 0.050 0.038 0.028 0.042 \n",
            "                        blocks.17.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.17.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.17.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.17.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.027 0.037 0.049 0.063 0.077 0.090 0.100 0.100 0.090 0.077 0.063 0.049 0.037 0.027 0.042 \n",
            "                            blocks.18.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.18.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.18.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.18.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.18.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                 blocks.18.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                     blocks.18.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.18.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.18.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.18.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.18.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.050 0.063 0.077 0.089 0.098 0.098 0.089 0.077 0.063 0.050 0.038 0.028 0.042 \n",
            "                            blocks.19.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.19.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.19.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.19.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.093 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.19.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                 blocks.19.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                     blocks.19.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.088 0.094 0.094 0.089 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.19.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.19.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.19.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.19.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.050 0.063 0.077 0.089 0.097 0.097 0.089 0.077 0.063 0.050 0.038 0.028 0.042 \n",
            "                            blocks.20.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.20.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.20.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.20.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.077 0.065 0.052 0.039 0.028 0.042 \n",
            "                      blocks.20.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.088 0.094 0.095 0.088 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                 blocks.20.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.078 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.20.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.088 0.094 0.095 0.089 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.20.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.20.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.20.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.20.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.050 0.063 0.077 0.089 0.097 0.097 0.089 0.077 0.063 0.050 0.038 0.028 0.042 \n",
            "                            blocks.21.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.21.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.21.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.21.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                      blocks.21.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.064 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.21.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.21.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.089 0.094 0.095 0.088 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.21.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.21.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.21.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.091 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.21.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.077 0.089 0.096 0.096 0.089 0.077 0.064 0.050 0.038 0.028 0.042 \n",
            "                            blocks.22.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.22.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.22.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.22.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.087 0.078 0.065 0.052 0.039 0.028 0.042 \n",
            "                      blocks.22.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.22.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.22.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.089 0.095 0.095 0.089 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.22.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.22.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.22.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.053 0.065 0.077 0.087 0.091 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.22.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.038 0.028 0.042 \n",
            "                            blocks.23.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.23.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.23.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.23.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.23.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.088 0.093 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                 blocks.23.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.23.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.089 0.095 0.095 0.089 0.078 0.064 0.051 0.038 0.028 0.042 \n",
            "                        blocks.23.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.23.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.23.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.23.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                            blocks.24.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.24.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.24.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.24.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.24.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                 blocks.24.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.24.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.064 0.078 0.088 0.094 0.095 0.088 0.078 0.065 0.051 0.038 0.028 0.042 \n",
            "                        blocks.24.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.24.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.24.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.24.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.051 0.064 0.077 0.088 0.094 0.094 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                            blocks.25.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.25.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.25.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.25.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.078 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.25.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                 blocks.25.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.25.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.038 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.25.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.25.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.25.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.053 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.25.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.077 0.088 0.094 0.094 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                            blocks.26.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.26.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.26.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.26.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.078 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.26.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                 blocks.26.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.26.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.26.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.26.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.26.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.091 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.26.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.077 0.088 0.093 0.093 0.088 0.077 0.065 0.051 0.039 0.028 0.042 \n",
            "                            blocks.27.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.27.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.27.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.27.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.27.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                 blocks.27.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.078 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.27.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.27.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.27.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.27.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.043 \n",
            "                      blocks.27.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.077 0.065 0.051 0.039 0.029 0.042 \n",
            "                            blocks.28.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.28.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.28.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.28.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.078 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.28.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.28.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.28.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.093 0.093 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.28.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.28.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.28.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.091 0.091 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.28.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                            blocks.29.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.29.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.29.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.29.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.078 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.29.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.29.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.29.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.052 0.065 0.078 0.088 0.093 0.093 0.088 0.077 0.065 0.052 0.039 0.028 0.042 \n",
            "                        blocks.29.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.29.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.29.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.29.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.077 0.088 0.094 0.094 0.088 0.077 0.065 0.051 0.039 0.029 0.042 \n",
            "                            blocks.30.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.30.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.30.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.30.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.039 0.029 0.042 \n",
            "                      blocks.30.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.30.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.029 0.039 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.30.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.077 0.088 0.093 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.30.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.30.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.086 0.091 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.30.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.053 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.30.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.073 0.028 0.039 0.051 0.064 0.077 0.088 0.095 0.095 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "                            blocks.31.ln1.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.31.ln1.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                            blocks.31.ln2.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                              blocks.31.ln2.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.time_decay - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.time_first - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.time_mix_v - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.att.key.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.039 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.31.att.value.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.31.att.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                     blocks.31.att.output.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.073 0.028 0.039 0.051 0.065 0.078 0.088 0.094 0.094 0.088 0.078 0.065 0.051 0.039 0.028 0.042 \n",
            "                        blocks.31.ffn.time_mix_k - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.ffn.time_mix_r - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                        blocks.31.ffn.key.weight - [ 4096, 16384], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                 blocks.31.ffn.receptance.weight - [ 4096,  4096], type =    F16 quantizing... size =    64.00 MB ->    12.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.092 0.092 0.086 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                      blocks.31.ffn.value.weight - [16384,  4096], type =    F16 quantizing... size =   256.00 MB ->    48.00 MB | hist: 0.074 0.029 0.040 0.052 0.065 0.077 0.087 0.093 0.093 0.087 0.077 0.065 0.052 0.040 0.029 0.042 \n",
            "                                   ln_out.weight - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                                     ln_out.bias - [ 4096,     1], type =    F32 size =    0.016 MB\n",
            "                                     head.weight - [ 4096, 50277], type =    F16 size =  392.789 MB\n",
            "original size     = 14103.14 MB\n",
            "quantized size    =  5783.14 MB\n",
            "compression ratio =     2.44%\n",
            "hist: 0.073 0.028 0.039 0.051 0.064 0.077 0.088 0.096 0.096 0.088 0.077 0.064 0.051 0.039 0.028 0.042 \n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对话应用\n",
        "!python rwkv/chat_with_bot.py rwkv.cpp-Raven-7B-Q4_1_O.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV_X2KUWRqm6",
        "outputId": "7559800c-b16a-43b7-b239-429bfa76247b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20B tokenizer\n",
            "System info: AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
            "Loading RWKV model\n",
            "Processing 92 prompt tokens, may take a while\n",
            "/content/drive/MyDrive/AIGC/chatrwkv/rwkv.cpp/rwkv/rwkv_cpp_model.py:100: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  state_out.storage().data_ptr(),\n",
            "/content/drive/MyDrive/AIGC/chatrwkv/rwkv.cpp/rwkv/rwkv_cpp_model.py:101: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  logits_out.storage().data_ptr()\n",
            "/content/drive/MyDrive/AIGC/chatrwkv/rwkv.cpp/rwkv/rwkv_cpp_model.py:82: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  state_in_ptr = state_in.storage().data_ptr()\n",
            "\n",
            "Chat initialized! Write something and press Enter.\n",
            "用python代码编写贪吃蛇小游戏\n",
            "> Bob: 请问有没有一种��言，可以实现��算��中的所有��作，比如��作文件，��开文件，����文件，��行程序，��行��本，��行����等等，都可以实现？\n",
            "用python编写等差数列求和函数\n",
            "> Bob: 请问有没有一种��言，可以实现��算��中的所有��作，比如��作文件，��开文件，����文件，��请你扮演一个文本冒险游戏，我是游戏主角。这是一个玄幻修真世界，有四大门派。我输入我的行动，请你显示行动结果，并具体描述环境。我的第一个行动是“醒来”，请开始故事。\n",
            "行程序，��行��本，��行����等等，都可以实现？\n",
            "> Bob: 请问有没有一种��言，可以实现��算��中的所有��作，比如��作文件，��开文件，����文件，��行程序，��行��本，��行����等等，都可以实现？\n",
            "请你扮演一个文本冒险游戏，我是游戏主角。这是一个玄幻修真世界，有四大门派。我输入我的行动，请你显示行动结果，并具体描述环境。我的第一个行动是“醒来”，请开始故事。\n",
            "> Bob: 请问有没有一种��言，可以实现��算��中的所有��作，比如��作文件，��开文件，����文件，��行程序，��行��本，��行����等等，都可以实现？\n"
          ]
        }
      ]
    }
  ]
}